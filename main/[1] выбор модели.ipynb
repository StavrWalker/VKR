{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1463f02-4a76-4f74-9e04-4c1c71156743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# tf.test.is_gpu_available(\n",
    "#     cuda_only=False, min_cuda_compute_capability=None\n",
    "# )\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print('Invalid device or cannot modify virtual devices once initialized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8511d0-9707-4671-bce0-7fa6ba26e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d68619-3485-48a2-b436-344c3d490aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Dropout, Conv1D, MaxPool1D, Flatten, UpSampling1D, AveragePooling1D#, Flatten\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.python.keras import utils\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c14fa0c5-abf3-44db-89b5-c67db3d06550",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset = '../dataset/N_DATA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3ae0ea6-68f3-48eb-be36-9fcc7da6cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    x, y = [], []\n",
    "    directory = os.fsencode(path_to_dataset)\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        pos = filename.find('_')\n",
    "        label = filename[pos+1:-4]\n",
    "        data = []\n",
    "        with open(path_to_dataset+'/'+filename, newline='') as f:\n",
    "            reader = csv.reader(f, delimiter=';')\n",
    "            data = [[float(y) for y in x] for x in list(reader)]\n",
    "        data = np.array(data).T.tolist()\n",
    "        data.pop(0)\n",
    "        data.pop(-1)\n",
    "        x.append(np.array(data).flatten())\n",
    "        y.append(int(label))\n",
    "    return np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48647860-2e9f-45da-99ec-07e00f07b1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(411, 400, 7) (137, 400, 7) (411,) (137,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Удалим первые нули чтобы форма была (400, 7) для сохранения формы после пуллинга\n",
    "'''\n",
    "x, y = load_dataset()\n",
    "x = np.array([point.reshape(401, 7) for point in x])\n",
    "x = np.array([point[1:] for point in x])\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9206f4d7-622d-419a-b0ea-38d9d002fdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(400, 7))\n",
    "\n",
    "def conv(kernels, kernel_size, pool, latent_dim=64, activation='relu'):\n",
    "    \n",
    "    encoded = Conv1D(kernels, kernel_size, activation = 'relu', padding = 'same')(input_layer)\n",
    "    encoded = MaxPool1D(pool, padding='same')(encoded)\n",
    "    \n",
    "    encoded = Conv1D(kernels, kernel_size, activation = 'relu', padding = 'same')(encoded)\n",
    "    encoded = MaxPool1D(pool, padding='same')(encoded)\n",
    "    \n",
    "#     encoded = Flatten()(encoded)\n",
    "#     encoded = Dense(latent_dim)(encoded)\n",
    "\n",
    "    decoded = Conv1D(kernels, kernel_size, activation = 'relu', padding = 'same')(encoded)\n",
    "    decoded = UpSampling1D(pool)(decoded)\n",
    "    \n",
    "    decoded = Conv1D(kernels, kernel_size, activation = 'relu', padding = 'same')(decoded)\n",
    "    decoded = UpSampling1D(pool)(decoded)\n",
    "    \n",
    "    decoded = Conv1D(7, kernel_size, activation = 'sigmoid', padding = 'same')(decoded)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a37995e-ad37-4553-9a57-8d85219d2f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 1 loss = 0.22165244817733765\n",
      "iter = 2 loss = 0.22199948132038116\n",
      "iter = 3 loss = 0.2219565212726593\n",
      "iter = 4 loss = 0.22168411314487457\n",
      "iter = 5 loss = 0.22172857820987701\n",
      "Average loss = 0.22180422842502595\n"
     ]
    }
   ],
   "source": [
    "pool = 2\n",
    "kernels = 17\n",
    "kernel_size = 7\n",
    "losses = []\n",
    "\n",
    "for i in range(5):\n",
    "    autoencoder = conv(kernels, kernel_size, pool)\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    history = autoencoder.fit(x_train, x_train,\n",
    "                                  epochs=160,\n",
    "                                  batch_size=16,\n",
    "                                  shuffle=True,\n",
    "                                  verbose=0,\n",
    "                                  validation_data=(x_test, x_test))\n",
    "    current_loss = history.history['val_loss'][-1]\n",
    "    losses.append(current_loss)\n",
    "    print(f'iter = {i+1} loss = {current_loss}')\n",
    "\n",
    "print(f'Average loss = {np.sum(losses) / 5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4ddd9e7-fcd0-46af-a324-e79c5246f607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(411, 2807) (137, 2807) (411,) (137,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Стандартная загрузка\n",
    "'''\n",
    "x, y = load_dataset()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77f9dca8-d0e3-4f9b-b9e5-89ef40a5d4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Input(shape=(2807,))\n",
    "\n",
    "def basic(n_layers, latent_dim, k=2, activation='relu'):\n",
    "    encoded = Dense(n_layers*latent_dim, activation=activation)(input_layer)\n",
    "    for current_dim in range(n_layers - 1, 0, -1):\n",
    "        encoded = Dense(current_dim*latent_dim, activation=activation)(encoded)\n",
    "    \n",
    "    if n_layers > 1:\n",
    "        decoded = Dense(2*latent_dim, activation=activation)(encoded)\n",
    "        for current_dim in range(3, n_layers + 1):\n",
    "            decoded = Dense(current_dim*latent_dim, activation=activation)(decoded)\n",
    "        decoded = Dense(2807, activation='sigmoid')(decoded)\n",
    "    else:\n",
    "         decoded = Dense(2807, activation='sigmoid')(encoded)\n",
    "    \n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9134e7b-cce8-4c60-8abb-aef42359ab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 1 loss = 0.22289322316646576\n",
      "iter = 2 loss = 0.2225545048713684\n",
      "iter = 3 loss = 0.22272862493991852\n",
      "iter = 4 loss = 0.22264377772808075\n",
      "iter = 5 loss = 0.22258934378623962\n",
      "Average loss = 0.2226818948984146\n"
     ]
    }
   ],
   "source": [
    "layers = 3\n",
    "latent_dim = 64\n",
    "losses = []\n",
    "\n",
    "for i in range(5):\n",
    "    autoencoder = basic(layers, latent_dim, activation='elu')\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    history = autoencoder.fit(x_train, x_train,\n",
    "                                  epochs=160,\n",
    "                                  batch_size=16,\n",
    "                                  shuffle=True,\n",
    "                                  verbose=0,\n",
    "                                  validation_data=(x_test, x_test))\n",
    "    current_loss = history.history['val_loss'][-1]\n",
    "    losses.append(current_loss)\n",
    "    print(f'iter = {i+1} loss = {current_loss}')\n",
    "\n",
    "print(f'Average loss = {np.sum(losses) / 5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1fa8925-c386-476e-8b13-bfe73732a917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 1 loss = 0.22221428155899048\n",
      "iter = 2 loss = 0.22217793762683868\n",
      "iter = 3 loss = 0.22226083278656006\n",
      "iter = 4 loss = 0.2222176343202591\n",
      "iter = 5 loss = 0.22231797873973846\n",
      "Average loss = 0.22223773300647737\n"
     ]
    }
   ],
   "source": [
    "layers = 2\n",
    "latent_dim = 128\n",
    "losses = []\n",
    "\n",
    "for i in range(5):\n",
    "    autoencoder = basic(layers, latent_dim, activation='elu')\n",
    "    autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    history = autoencoder.fit(x_train, x_train,\n",
    "                                  epochs=160,\n",
    "                                  batch_size=16,\n",
    "                                  shuffle=True,\n",
    "                                  verbose=0,\n",
    "                                  validation_data=(x_test, x_test))\n",
    "    current_loss = history.history['val_loss'][-1]\n",
    "    losses.append(current_loss)\n",
    "    print(f'iter = {i+1} loss = {current_loss}')\n",
    "\n",
    "print(f'Average loss = {np.sum(losses) / 5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf06808-94bf-4996-8fba-9410ed4f3e6c",
   "metadata": {},
   "source": [
    "|            | Число обучаемых параметров | Число выделяемых признаков | Средняя ошибка |\n",
    "|------------|----------------------------|----------------------------|----------------|\n",
    "| Сверточная | 7810                       | 1700                       | 0.22180        |\n",
    "| 3 слоя     | 1146935                    | 64                         | 0.22268        |\n",
    "| 2 слоя     | 1506167                    | 128                        | 0.22224        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2506d109-0680-4056-a7fb-31482929d354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
